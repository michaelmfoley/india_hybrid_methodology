{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "hybrid = gpd.read_file('/Users/michaelfoley/Google Drive/My Drive/Subnational_Yield_Database/boundaries/country/IND/2016_2023_hybrid_boundary.shp')\n",
    "ag_stats = pd.read_csv('/Users/michaelfoley/Library/CloudStorage/GoogleDrive-mfoley@g.harvard.edu/My Drive/Subnational_Yield_Database/data/processed/IND/ag_stats_final_nozeros_031025.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import chardet\n",
    "with open('/Users/michaelfoley/Library/CloudStorage/GoogleDrive-mfoley@g.harvard.edu/My Drive/Subnational_Yield_Database/data/processed/IND/ag_stats_final_031025.csv', 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(10000))\n",
    "print(f\"Detected encoding: {result['encoding']} with confidence {result['confidence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "ag_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "hybrid.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifications\n",
    "- drop columns we don't care about\n",
    "    -columns to keep\n",
    "        -id, boundary, name, wikidata, wikipedia, merged_dis, combined, group_numb, multidist, change_yea\n",
    "- make a separate column that assigns the state to each district\n",
    "- check to see why number of district names is different than the number of district names in the relationship table in 2016\n",
    "- apply state abbreviation to every district"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also check to see why the number of district names is different than the number of district names in the relationship table in 2016. Next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hybrid = hybrid[['id', 'boundary', 'name', 'STATE', 'name_state', 'wikidata', 'wikipedia', 'merged_dis', 'combined', 'group_numb', 'multidistr', 'change_yea', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "filtered_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "ag_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "maize_test_2019 = ag_stats[(ag_stats['Year'] == 2019) & (ag_stats['Source crop'] == 'Maize')]\n",
    "maize_test_2021 = ag_stats[(ag_stats['Year'] == 2021) & (ag_stats['Source crop'] == 'Maize')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "maize_test_2019.value_counts('Season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "maize_test_2021.value_counts('Season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "np.quantile(maize_test_2019[maize_test_2019['Season'] == 'Autumn']['Yield: MT/ha'], [0, 0.25, 0.5, 0.75, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "np.quantile(maize_test_2021[maize_test_2021['Season'] == 'Autumn']['Yield: MT/ha'], [0, 0.25, 0.5, 0.75, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "maize_test_2021[maize_test_2021['Season'] == 'Autumn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "len(maize_test_2019[maize_test_2019['Season'] == 'Autumn']['Yield: MT/ha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plt.hist(maize_test_2019[maize_test_2019['Season'] == 'Autumn']['Yield: MT/ha'], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plt.hist(maize_test_2021[maize_test_2021['Season'] == 'Autumn']['Yield: MT/ha'], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "np.count_nonzero(maize_test_2019[(maize_test_2019['Season'] == 'Kharif')]['Yield: MT/ha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "np.count_nonzero(maize_test_2021[(maize_test_2021['Season'] == 'Kharif')]['Yield: MT/ha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter change boundary names to help aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping hybrid to crops\n",
    "#decide what to do with Hanamkonda - left as Warangal Urban (TG) for now as that is how it is represented in the data, so I have some duplicates because of it.\n",
    "#maybe rename during data aggregation phase? Same with prayagraj - look into. And kalaburagi, though both prayagraj and kalaburagi were changed before 2016 so they should be fine.\n",
    "#missed Shi Yomi division in 2018 from West Siang\n",
    "hybrid_mapping = {\n",
    "    'Prayagraj (UP)': 'Allahabad (UP)', \n",
    "    'Aurangabad (MH) (MH)': 'Aurangabad (MH)',\n",
    "    'Budgam (JK)': 'Badgam (JK)',\n",
    "    'Bandipora (JK)': 'Bandipore (JK)',\n",
    "    'Baramulla (JK)': 'Baramula (JK)', \n",
    "    'Ballari (KA)': 'Bellary (KA)',\n",
    "    'Charkhi Dadri (HR)': 'Charki Dadri (HR)', #check what this one became\n",
    "    'Siddarth Nagar (UP)': 'Siddharth Nagar (UP)',\n",
    "    'Kalaburagi (KA)': 'Gulbarga (KA)',\n",
    "    'South Sikkhim (SK)': 'South Sikkim (SK)',\n",
    "    'Shivamogga (KA)': 'Shimoga (KA)',\n",
    "}\n",
    "\n",
    "merged_dis_mapping = {\n",
    "    'Bhiwani (HR), Charkhi Dadri (HR)': 'Bhiwani (HR), Charki Dadri (HR)', #check what this one became\n",
    "}\n",
    "\n",
    "ag_mapping = {\n",
    "    'Siaha (MZ)': 'Saiha (MZ)',\n",
    "}\n",
    "\n",
    "filtered_hybrid['name_state'] = filtered_hybrid['name_state'].replace(hybrid_mapping)\n",
    "filtered_hybrid['merged_dis'] = filtered_hybrid['merged_dis'].replace(merged_dis_mapping)\n",
    "ag_stats['Admin 2'] = ag_stats['Admin 2'].replace(ag_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we try to aggregate all of the data. To do so, we will cycle through each of the groups for each year and aggregate any name matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter crop data for years 2016 onwards\n",
    "hybrid_stats = ag_stats[ag_stats['Year'] >= 2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_extra_value(s):\n",
    "    \"\"\"\n",
    "    Returns the first non-null value if all non-null values in s are identical.\n",
    "    If there are conflicting non-null values, returns the first value.\n",
    "    \"\"\"\n",
    "    unique_vals = s.dropna().unique()\n",
    "    if len(unique_vals) == 1:\n",
    "        return unique_vals[0]\n",
    "    elif len(unique_vals) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return unique_vals[0]\n",
    "\n",
    "def aggregate_extra_flag(s):\n",
    "    \"\"\"\n",
    "    Returns True if there is more than one unique non-null value in s,\n",
    "    indicating that the values are inconsistent.\n",
    "    \"\"\"\n",
    "    unique_vals = s.dropna().unique()\n",
    "    return len(unique_vals) > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data_for_polygon(districts, crop_df, polygon_name):\n",
    "    \"\"\"\n",
    "    Aggregates crop data for a list of districts belonging to one polygon.\n",
    "    \n",
    "    Grouping is done by Year, CropType, and Season.\n",
    "      - Sums AreaHarvested and QuantityProduced.\n",
    "      - Computes weighted yield (using AreaHarvested as the weight).\n",
    "      \n",
    "    For extra (metadata) columns, we aggregate by taking the first non-null value,\n",
    "    and we also compute a flag (e.g., Source_flag) that is True if there are conflicting values.\n",
    "    \n",
    "    For each aggregated group where any flag is True, we assign a unique agg_group_id and\n",
    "    extract the original rows for that group for further inspection.\n",
    "    \n",
    "    Returns a tuple of:\n",
    "      - The aggregated DataFrame (with an added 'agg_group_id' column for flagged groups).\n",
    "      - A DataFrame of flagged rows from the original crop data, each with an 'agg_group_id' that connects it to the aggregated row.\n",
    "      - A list of indices from crop_df that were aggregated.\n",
    "    \"\"\"\n",
    "    global group_counter\n",
    "    \n",
    "    # Filter crop data for matching districts\n",
    "    subset = crop_df[crop_df['Admin 2'].isin(districts)].copy()\n",
    "    if subset.empty:\n",
    "        return None, pd.DataFrame(), []\n",
    "    \n",
    "    # Record indices being aggregated (for later removal from df_crop)\n",
    "    aggregated_indices = subset.index.tolist()\n",
    "    \n",
    "    # Helper column for weighted yield\n",
    "    subset['yield_weighted'] = subset['Yield: MT/ha'] * subset['Area Harvested: ha']\n",
    "    \n",
    "    # Grouping keys: only aggregate same Year, CropType, Season\n",
    "    group_keys = ['Year', 'Source crop', 'Season']\n",
    "    \n",
    "    # Numeric columns we aggregate\n",
    "    numeric_cols = ['Area Harvested: ha', 'Quantity Produced: MT', 'Yield: MT/ha', 'yield_weighted']\n",
    "    # Extra (metadata) columns: exclude grouping keys, numeric columns, and District identifier\n",
    "    extra_cols = [col for col in subset.columns if col not in group_keys + numeric_cols + ['Admin 2']]\n",
    "    \n",
    "    # Group the subset by the grouping keys\n",
    "    grouped = subset.groupby(group_keys, as_index=False)\n",
    "    \n",
    "    # Aggregate numeric values\n",
    "    agg = grouped.agg(\n",
    "        area_harvested=('Area Harvested: ha', 'sum'),\n",
    "        quantity_produced=('Quantity Produced: MT', 'sum'),\n",
    "        yield_weighted_sum=('yield_weighted', 'sum')\n",
    "    )\n",
    "    # Compute weighted yield for each group\n",
    "    agg['weighted_yield'] = agg['yield_weighted_sum'] / agg['area_harvested']\n",
    "    agg.drop(columns='yield_weighted_sum', inplace=True)\n",
    "    \n",
    "    # Aggregate extra columns and compute flags\n",
    "    if extra_cols:\n",
    "        agg_extra = grouped.agg({col: lambda s: aggregate_extra_value(s) for col in extra_cols}).reset_index()\n",
    "        agg_flags = grouped.agg({col: lambda s: aggregate_extra_flag(s) for col in extra_cols}).reset_index()\n",
    "        # Rename flag columns to add _flag suffix\n",
    "        agg_flags.rename(columns={col: f\"{col}_flag\" for col in extra_cols}, inplace=True)\n",
    "        extra = pd.merge(agg_extra, agg_flags, on=group_keys)\n",
    "        # Merge extra aggregated info into agg\n",
    "        agg = pd.merge(agg, extra, on=group_keys, how='left')\n",
    "    \n",
    "    # Add polygon_name to aggregated data\n",
    "    agg['polygon_name'] = polygon_name\n",
    "    \n",
    "    # Create a column to hold the aggregated group ID (for flagged groups)\n",
    "    agg['agg_group_id'] = None\n",
    "    flagged_rows = []  # List to collect flagged original rows\n",
    "    \n",
    "    # Identify which columns are the flag columns\n",
    "    flag_columns = [col for col in agg.columns if col.endswith('_flag')]\n",
    "    \n",
    "    # For each aggregated group, if any flag is True, record the group ID and extract the original rows\n",
    "    for i, row in agg.iterrows():\n",
    "        # Check if any flag is True in this group\n",
    "        flags = [row[flag] for flag in flag_columns if flag in row]\n",
    "        if any(flags):\n",
    "            # Assign a unique group id\n",
    "            group_id = f\"G{group_counter}\"\n",
    "            group_counter += 1\n",
    "            # Set the group id for the aggregated row\n",
    "            agg.at[i, 'agg_group_id'] = group_id\n",
    "            # Extract all original rows from subset matching this group\n",
    "            # (match on each grouping key: Year, CropType, Season)\n",
    "            condition = True\n",
    "            for key in group_keys:\n",
    "                condition = condition & (subset[key] == row[key])\n",
    "            flagged_group = subset[condition].copy()\n",
    "            flagged_group['agg_group_id'] = group_id\n",
    "            flagged_group['polygon_name'] = polygon_name\n",
    "            flagged_rows.append(flagged_group)\n",
    "    \n",
    "    if flagged_rows:\n",
    "        flagged_df = pd.concat(flagged_rows, ignore_index=True)\n",
    "    else:\n",
    "        flagged_df = pd.DataFrame()\n",
    "    \n",
    "    return agg, flagged_df, aggregated_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Global counter to assign unique aggregated group IDs for flagged groups\n",
    "group_counter = 1\n",
    "\n",
    "# Lists to collect results across all polygons\n",
    "aggregated_results = []\n",
    "flagged_results = []\n",
    "all_aggregated_indices = set()\n",
    "\n",
    "# Iterate over each polygon in the shapefile\n",
    "for idx, poly in filtered_hybrid.iterrows():\n",
    "    district_names = poly.get('merged_dis', None)\n",
    "    if not district_names:\n",
    "        print(f\"Skipping polygon {poly.get('id', idx)} because 'merged_dis' is missing.\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Aggregating {poly.get('merged_dis')}\")\n",
    "\n",
    "    district_list = [name.strip() for name in district_names.split(',')]\n",
    "    polygon_name = poly.get('name_state', poly.get('id', idx))\n",
    "    \n",
    "    agg_data, flagged_data, indices = aggregate_data_for_polygon(district_list, hybrid_stats, polygon_name)\n",
    "    if agg_data is not None:\n",
    "        aggregated_results.append(agg_data)\n",
    "        flagged_results.append(flagged_data)\n",
    "        all_aggregated_indices.update(indices)\n",
    "\n",
    "# Combine aggregated results from all polygons into one DataFrame\n",
    "if aggregated_results:\n",
    "    agg_df = pd.concat(aggregated_results, ignore_index=True)\n",
    "else:\n",
    "    agg_df = pd.DataFrame()\n",
    "\n",
    "# Combine all flagged rows from polygons into one DataFrame\n",
    "if flagged_results:\n",
    "    flagged_df = pd.concat(flagged_results, ignore_index=True)\n",
    "else:\n",
    "    flagged_df = pd.DataFrame()\n",
    "\n",
    "# Remove the aggregated rows from the original crop data\n",
    "remaining_crop = hybrid_stats.drop(index=all_aggregated_indices).copy()\n",
    "\n",
    "# Optionally add a polygon_name column to remaining rows if desired\n",
    "if 'polygon_name' not in remaining_crop.columns:\n",
    "    remaining_crop['polygon_name'] = None\n",
    "\n",
    "# Create final DataFrame: aggregated rows followed by non-aggregated rows.\n",
    "final_df = pd.concat([agg_df, remaining_crop], ignore_index=True)\n",
    "\n",
    "# Now you have:\n",
    "#  - agg_df: Aggregated crop data (with agg_group_id for flagged groups)\n",
    "#  - flagged_df: Original rows (with agg_group_id) that had inconsistent extra values\n",
    "#  - final_df: Full crop data with aggregated rows replacing original ones, ready to merge back into the shapefile.\n",
    "\n",
    "print(\"Aggregated DataFrame (agg_df):\")\n",
    "print(agg_df.head())\n",
    "\n",
    "print(\"\\nFlagged Rows for Further Inspection (flagged_df):\")\n",
    "print(flagged_df.head())\n",
    "\n",
    "print(\"\\nFinal DataFrame (final_df):\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "flagged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "final_df['Area Harvested: ha'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "final_df['area_harvested'].notna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. All we need to do now is merge those columns, apply consistent formating to yields to ensure no super long decimals, remove flag columns, and test compared to icrisat yields from 2016 visually. Also need to remove:\n",
    "- Budgam (JK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[final_df['Admin 2'] != 'Budgam (JK)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifications to main data table\n",
    " - Need to fix Rabi season in main data table - should go to 1/31/[next year] [DONE]\n",
    " - Remove zeros [DONE]\n",
    " - Standardize source year [DONE]\n",
    " - Summer Ballari start and end dates 2022 need year fixed (currently /23)\n",
    " - fix source document (can do later)\n",
    "\n",
    "# Modifications to boundary file\n",
    " - every name should be stored with it's appropriate state name to avoid confusion (particularly Bilaspur). This is tough though when states change. [DONE]\n",
    " - Must clarify bijapur, hamirpur, and pratapgarh [DONE]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try merging aggregated area harvested with original area harvested\n",
    "final_df['Area Harvested: ha'] = final_df['Area Harvested: ha'].fillna(final_df['area_harvested'])\n",
    "final_df['Quantity Produced: MT'] = final_df['Quantity Produced: MT'].fillna(final_df['quantity_produced'])\n",
    "final_df['Yield: MT/ha'] = final_df['Yield: MT/ha'].fillna(final_df['weighted_yield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in final_df.columns:\n",
    "    if 'flag' in column:\n",
    "        final_df.drop(columns=column, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "final_df['Admin 2'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "final_df['polygon_name'].notna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge polygon_name to Admin 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Admin 2'] = final_df['Admin 2'].fillna(final_df['polygon_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sort = ['Data Source Organization',\t'Data Source Document',\t'Publication Name', 'Survey Type', 'Country', 'Zone', 'FNID', \n",
    "'Admin 1', 'Admin 2', 'Admin 3', 'Year', 'Start Period', 'Season', 'End Period', 'Crop', 'Dominant Production System', \n",
    "'Area Planted: ha', 'Area Harvested: ha', 'Yield: MT/ha', 'Quantity Produced: MT', 'Contributions by', 'Source crop', 'DNL_Source_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[column_sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('/Users/michaelfoley/Library/CloudStorage/GoogleDrive-mfoley@g.harvard.edu/My Drive/Subnational_Yield_Database/data/processed/IND/2016_2023_hybrid_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge hybrid boundary and this data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hybrid.to_file('/Users/michaelfoley/Library/CloudStorage/GoogleDrive-mfoley@g.harvard.edu/My Drive/Subnational_Yield_Database/data/processed/IND/2016_2023_hybrid_boundary.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "merged_ourdata = final_df.merge(filtered_hybrid, left_on='Admin 2', right_on='name_state', how='left')\n",
    "merged_ourdata = gpd.GeoDataFrame(merged_ourdata, geometry='geometry')\n",
    "\n",
    "# Check if merge was successful\n",
    "print(f\"Original CSV DataFrame shape: {final_df.shape}\")\n",
    "print(f\"Merged GeoDataFrame shape: {merged_ourdata.shape}\")\n",
    "print(f\"Number of rows with geometry: {merged_ourdata.geometry.notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_ourdata.to_file('/Users/michaelfoley/Library/CloudStorage/GoogleDrive-mfoley@g.harvard.edu/My Drive/Subnational_Yield_Database/data/processed/IND/hybrid_data_2016_2023.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Count rows missing geometry\n",
    "missing_geometry = merged_ourdata[merged_ourdata.geometry.isna()]\n",
    "print(f\"Number of rows missing geometry: {len(missing_geometry)}\")\n",
    "\n",
    "# Look at some examples of rows missing geometry\n",
    "sorted(missing_geometry['Admin 2'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "missing_geometry['Admin 2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_check(old_df, final_df):\n",
    "    # Check for overlap\n",
    "    check_columns = ['Year', 'Source crop', 'Season', 'Area Harvested: ha', 'Yield: MT/ha', 'Quantity Produced: MT']\n",
    "    \n",
    "    # Convert year in test dataframe (already float for others)\n",
    "    old_df['Year'] = old_df['Year'].astype(int)\n",
    "    old_df['Area Harvested: ha'] = pd.to_numeric(old_df['Area Harvested: ha'], errors='coerce').round(2)\n",
    "    old_df['Yield: MT/ha'] = pd.to_numeric(old_df['Yield: MT/ha'], errors='coerce').round(2)\n",
    "    old_df['Quantity Produced: MT'] = pd.to_numeric(old_df['Quantity Produced: MT'], errors='coerce').round(2)\n",
    "\n",
    "    # Now try the merge\n",
    "    matches = final_df.merge(\n",
    "        old_df, \n",
    "        on=check_columns,\n",
    "        indicator=True,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Count matches and non-matches\n",
    "    match_stats = matches['_merge'].value_counts()\n",
    "\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Total rows in test dataset: {len(final_df)}\")\n",
    "    print(f\"Number of matching rows: {sum(matches['_merge'] == 'both')}\")\n",
    "    print(f\"Number of non-matching rows: {sum(matches['_merge'] == 'left_only')}\")\n",
    "    print(f\"Percentage of test rows found in main dataset: {(sum(matches['_merge'] == 'both') / len(final_df)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "shivamogga = merged_ourdata[merged_ourdata['Admin 2'] == 'Shivamogga (KA)']\n",
    "shimoga = merged_ourdata[merged_ourdata['Admin 2'] == 'Shimoga (KA)']\n",
    "kadapa = merged_ourdata[merged_ourdata['Admin 2'] == 'Kalaburagi (KA)']\n",
    "ysr_kadapa = merged_ourdata[merged_ourdata['Admin 2'] == 'Gulbarga (KA)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "overlap_check(kadapa, ysr_kadapa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "missing_geometry.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "sorted(hybrid['name_state'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_2017_rice = merged_ourdata[(merged_ourdata['Year'] == 2017) & (merged_ourdata['Source crop'] == 'Rice')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "merged_2017_rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "merged_2017_rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "missing_geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_names_from_final = final_df['Admin 2'].dropna().unique()\n",
    "filtered_hybrid_new = filtered_hybrid[filtered_hybrid['name_state'].isin(valid_names_from_final)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20,20))\n",
    "\n",
    "merged_2017_rice.plot(ax = ax,\n",
    "    column='Yield: MT/ha',\n",
    "    legend=True,\n",
    "    vmin = 0,\n",
    "    vmax = 4)\n",
    "\n",
    "\n",
    "filtered_hybrid_new.boundary.plot(ax = ax,\n",
    "    linewidth=1.0, \n",
    "    edgecolor='black'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets make a map to compare ICRISAT yield data to our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "icrisat_path = '/Users/michaelfoley/Library/CloudStorage/GoogleDrive-mfoley@g.harvard.edu/My Drive/Subnational_Yield_Database/data/processed/IND/icrisat_apportioned/'\n",
    "shape_path = icrisat_path + 'icrisat_boundary_match.shp'\n",
    "data_path = icrisat_path + 'ICRISAT-District Level Data_Apportioned.csv'\n",
    "\n",
    "# Read the shapefile of district boundaries\n",
    "icrisat_districts = gpd.read_file(shape_path)\n",
    "\n",
    "# Read the CSV containing crop data (which includes columns like 'district_name', 'year', 'crop', 'area', 'production', 'yield')\n",
    "icrisat_data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a temporary dataframe with only the columns we need for joining\n",
    "# plus the geometry from the shapefile\n",
    "icrisat_lookup = icrisat_districts[['NAME_1', 'Dist_Name', 'geometry']]\n",
    "\n",
    "icrisat_lookup = icrisat_lookup.rename(columns={\n",
    "    'NAME_1': 'State Name',\n",
    "    'Dist_Name': 'Dist Name'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Now merge the dataframes\n",
    "icrisat_df = icrisat_data.merge(\n",
    "    icrisat_lookup,\n",
    "    on=['State Name', 'Dist Name'],\n",
    "    how='left'  # Use 'left' to keep all spatial features, 'inner' for only matching rows\n",
    ")\n",
    "\n",
    "# Convert the result to a GeoDataFrame\n",
    "icrisat_gdf = gpd.GeoDataFrame(icrisat_df, geometry='geometry')\n",
    "\n",
    "# Check if merge was successful\n",
    "print(f\"Original CSV DataFrame shape: {icrisat_data.shape}\")\n",
    "print(f\"Merged GeoDataFrame shape: {icrisat_gdf.shape}\")\n",
    "print(f\"Number of rows with geometry: {icrisat_gdf.geometry.notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Count rows missing geometry\n",
    "missing_geometry = icrisat_gdf[icrisat_gdf.geometry.isna()]\n",
    "print(f\"Number of rows missing geometry: {len(missing_geometry)}\")\n",
    "\n",
    "# Look at some examples of rows missing geometry\n",
    "missing_geometry[['State Name', 'Dist Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "icrisat_2016 = icrisat_gdf[icrisat_gdf['Year'] == 2016]\n",
    "icrisat_2016.plot(column = 'RICE YIELD (Kg per ha)',\n",
    "    vmin = 0,\n",
    "    vmax = 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_2016 = final_df[(final_df['Year'] == 2016) & (final_df['Source crop'] == 'Rice')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
